{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samieahmad/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# NLTK-specific download\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Datasets\n",
    "\n",
    "We are dealing with these datasets:\n",
    "\n",
    "1. **Golf Dataset**: This dataset aims to explore factors that influence the decision to play golf, which could be valuable for predictive modeling tasks. ​​\n",
    "\n",
    "2. **Tweet Evaluation Dataset**: Instead of downloading the dataset manually, we will be using the [`datasets`](https://huggingface.co/docs/datasets) library from Hugging Face to automatically download and manage the Tweet Eval dataset. This library is part of the Hugging Face ecosystem, widely used for Natural Language Processing (NLP) tasks. The `datasets` library not only downloads the dataset but also offers a standardized interface for accessing and handling the data, making it compatible with other popular libraries like Pandas and PyTorch. Format each split of the dataset into a Pandas DataFrame. The columns should be `text` and `label`, where `text` is the sentence and `label` is the emotion label. The goal is to classify tweets into various emotional categories (e.g., joy, sadness, anger) by analyzing their content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "golf_data = pd.read_csv('golf_data.csv')  # Replace with correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0735dea33ec949e2bcd8a7dd7967d43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff67c3a38bd4f67be3713397fa83151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ef07c97545463dafe302dadd1e0a3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# code here\n",
    "tweet_data = load_dataset('tweet_eval', 'emotion', cache_dir=\"datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before proceeding with further tasks, ensure you have determined which type of Naive Bayes is most suitable for each dataset.\n",
    "\n",
    "\n",
    "We will use **Multinomial Naive Bayes** for golf dataset and **Bernoulli Naive Bayes** for the tweet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tweets train data:  (3257, 2)\n",
      "Shape of tweets validation data:  (374, 2)\n",
      "Shape of tweets test data:  (1421, 2)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.DataFrame(tweet_data['train'])\n",
    "train_data.to_csv('emotion_train_data.csv', index=False)\n",
    "\n",
    "validation_data = pd.DataFrame(tweet_data['validation'])\n",
    "validation_data.to_csv('emotion_validation_data.csv', index=False)\n",
    "\n",
    "test_data = pd.DataFrame(tweet_data['test'])\n",
    "test_data.to_csv('emotion_test_data.csv', index=False)\n",
    "\n",
    "print(\"Shape of tweets train data: \", train_data.shape)\n",
    "print(\"Shape of tweets validation data: \", validation_data.shape)\n",
    "print(\"Shape of tweets test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing the Golf Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying one hot encoding and using `sklearn's` `train_test_split` to split the data, keeping test size = 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (5365, 8), (5365,)\n",
      "Test dataset shape: (2300, 8), (2300,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# One hot encoding values using the 'map' function\n",
    "golf_data['Month'] = golf_data['Month'].map({'Winter': 1, 'Non-Winter': 0})\n",
    "golf_data['Season'] = golf_data['Season'].map({'Winter': 1, 'Non-Winter': 0})\n",
    "golf_data['Temperature'] = golf_data['Temperature'].map({'high': 1, 'low': 0})\n",
    "golf_data['Humidity'] = golf_data['Humidity'].map({'high': 1, 'low': 0})\n",
    "golf_data['Outlook'] = golf_data['Outlook'].map({'sunny': 1, 'not sunny': 0})\n",
    "golf_data['Crowdedness'] = golf_data['Crowdedness'].map({'high': 1, 'not high': 0})\n",
    "\n",
    "# Split the features (X) and target (y)\n",
    "X = golf_data.drop(columns='Play')\n",
    "y = golf_data['Play']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Training dataset shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test dataset shape: {X_test.shape}, {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing the Tweet Eval Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pre-process the data to ensure it's in a clean format for further analysis. The following steps should be performed:\n",
    "\n",
    "- Remove any URL.\n",
    "- Remove punctuation and non-alphanumeric characters.\n",
    "- Convert all text to lowercase.\n",
    "- Remove any extra whitespace.\n",
    "- Eliminate common stopwords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/samieahmad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_dataset(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text,)      #URLs\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)      #Punctuation and non-alphanumeric characters\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   #Extra whitespaces\n",
    "    stop_words = set(stopwords.words('english'))  # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "train_data['processed_text'] = train_data['text'].apply(preprocess_dataset)\n",
    "validation_data['processed_text'] = validation_data['text'].apply(preprocess_dataset)\n",
    "test_data['processed_text'] = test_data['text'].apply(preprocess_dataset)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementing Naive Bayes from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bernoulli Naive Bayes\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "Recall that the Bernoulli Naive Bayes model is based on **Bayes' Theorem**:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class \\(c\\) that maximizes \\(P(c \\mid x)\\), so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "In the case of **Bernoulli Naive Bayes**, we assume that each word \\(x_i\\) in a sentence follows a **Bernoulli distribution**, meaning that the word either appears (1) or does not appear (0) in the document. We can simplify the formula using this assumption:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i = 1 \\mid c)^{x_i} P(x_i = 0 \\mid c)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_i = 1$ if the $i^{\\text{th}}$ word is present in the document.\n",
    "- $x_i = 0$ if the $i^{\\text{th}}$ word is not present in the document.\n",
    "\n",
    "\n",
    "We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i = 1 \\mid c)$ by counting the number of documents in class $c$ that contain the word $x_i$, and dividing by the total number of documents in class $c$.\n",
    "\n",
    "### **Important: Laplace Smoothing**\n",
    "\n",
    "When calculating $P(x_i = 1 \\mid c)$ and $P(x_i = 0 \\mid c)$, we apply **Laplace smoothing** to avoid zero probabilities. This is essential because, without it, any word that has not appeared in a document of class $c$ will have a probability of zero, which would make the overall product zero, leading to incorrect classification.\n",
    "\n",
    "**Reason**: Laplace smoothing ensures that we don't encounter zero probabilities by adding a small constant (typically 1) to both the numerator and the denominator. This is particularly useful when a word has never appeared in the training data for a specific class.\n",
    "\n",
    "The smoothed probability formula is:\n",
    "\n",
    "$$\n",
    "P(x_i = 1 \\mid c) = \\frac{\\text{count of documents in class } c \\text{ where } x_i = 1 + 1}{\\text{total documents in class } c + 2}\n",
    "$$\n",
    "\n",
    "This ensures no word has a zero probability, even if it was unseen in the training data.\n",
    "\n",
    "### Avoiding Underflow with Logarithms:\n",
    "\n",
    "To avoid underflow errors due to multiplying small probabilities, we apply logarithms, which convert the product into a sum:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\left[ x_i \\log P(x_i = 1 \\mid c) + (1 - x_i) \\log P(x_i = 0 \\mid c) \\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate the priors for each class\n",
    "def calculatePriors(y_train):\n",
    "    unique_classes, counts = np.unique(y_train, return_counts=True)\n",
    "    length = len(y_train)\n",
    "    priors = counts / length\n",
    "    return dict(zip(unique_classes, priors))\n",
    "\n",
    "#Calculate the likelihoods for each class with Laplace smoothing\n",
    "def calculateLikelihoods(X_train, y_train):\n",
    "    unique_classes = np.unique(y_train)\n",
    "    likelihoods = {}\n",
    "\n",
    "    for c in unique_classes:\n",
    "        class_indices = np.where(y_train == c)[0]\n",
    "        class_data = X_train[class_indices]  # Filter the data for class c\n",
    "\n",
    "        feature_sums = class_data.sum(axis=0)\n",
    "        smoothed_probabilities = (feature_sums + 1) / (len(class_data) + 2)  # Add 1 for Laplace smoothing\n",
    "        likelihoods[c] = smoothed_probabilities\n",
    "    \n",
    "    return likelihoods\n",
    "\n",
    "\n",
    "def bnbClassifier(test_data, priors, likelihoods):\n",
    "    unique_classes = list(priors.keys())  # Get unique class labels\n",
    "    predictions = []\n",
    "    \n",
    "    for sample in test_data:\n",
    "        log_posteriors = []  # Stores posterior probabilities for each class\n",
    "        \n",
    "        for c in unique_classes:\n",
    "            log_prior = np.log(priors[c])\n",
    "\n",
    "            log_likelihood_present = np.log(likelihoods[c]) * sample\n",
    "            log_likelihood_absent = np.log(1 - likelihoods[c]) * (1 - sample)\n",
    "            log_likelihood_total = np.sum(log_likelihood_present + log_likelihood_absent)\n",
    "\n",
    "            log_posterior = log_prior + log_likelihood_total\n",
    "            log_posteriors.append(log_posterior)\n",
    "\n",
    "        predicted_class = unique_classes[np.argmax(log_posteriors)]\n",
    "        predictions.append(predicted_class)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code block contains function definitions\n",
    "\n",
    "priors = calculatePriors(np.array(y_train))\n",
    "likelihoods = calculateLikelihoods(np.array(X_train), np.array(y_train))\n",
    "y_predicted = bnbClassifier(np.array(X_test), priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a Naive Bayes model on the training data, and generate predictions for the Validation Set and will also report the metrics for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.820\n",
      "\n",
      "Precision (Class 0): 0.824\n",
      "Precision (Class 1): 0.519\n",
      "\n",
      "Recall (Class 0): 0.993\n",
      "Recall (Class 1): 0.034\n",
      "\n",
      "F1 Score (Class 0): 0.900\n",
      "F1 Score (Class 1): 0.063\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1872   13]\n",
      " [ 401   14]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_score(y_test, y_predicted)\n",
    "\n",
    "precision_class0 = precision_score(y_test, y_predicted, pos_label=0)\n",
    "recall_class0 = recall_score(y_test, y_predicted, pos_label=0)\n",
    "f1_score_class0 = f1_score(y_test, y_predicted, pos_label=0)\n",
    "\n",
    "precision_class1 = precision_score(y_test, y_predicted, pos_label=1)\n",
    "recall_class1 = recall_score(y_test, y_predicted, pos_label=1)\n",
    "f1_score_class1 = f1_score(y_test, y_predicted, pos_label=1)\n",
    "\n",
    "# Confusion matrix remains the same\n",
    "confusion_matrix = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "# Report the results for class 0\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print()\n",
    "print(f\"Precision (Class 0): {precision_class0:.3f}\")\n",
    "print(f\"Precision (Class 1): {precision_class1:.3f}\")\n",
    "print()\n",
    "print(f\"Recall (Class 0): {recall_class0:.3f}\")\n",
    "print(f\"Recall (Class 1): {recall_class1:.3f}\")\n",
    "print()\n",
    "print(f\"F1 Score (Class 0): {f1_score_class0:.3f}\")\n",
    "print(f\"F1 Score (Class 1): {f1_score_class1:.3f}\")\n",
    "print()\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multinomial Naive Bayes (Manual Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing sentences with Bag of Words\n",
    "\n",
    "Now that we have loaded in our data, we will need to vectorize our sentences - this is necessary to be able to numericalize our inputs before feeding them into our model. \n",
    "\n",
    "We will be using a Bag of Words approach to vectorize our sentences. This is a simple approach that counts the number of times each word appears in a sentence. \n",
    "\n",
    "The element at index $\\text{i}$ of the vector will be the number of times the $\\text{i}^{\\text{th}}$ word in our vocabulary appears in the sentence. So, for example, if our vocabulary is `[\"the\", \"cat\", \"sat\", \"on\", \"mat\"]`, and our sentence is `\"the cat sat on the mat\"`, then our vector will be `[2, 1, 1, 1, 1]`.\n",
    "\n",
    "We will now create a `BagOfWords` class to vectorize our sentences. This will involve creating\n",
    "\n",
    "1. A vocabulary from our corpus\n",
    "\n",
    "2. A mapping from words to indices in our vocabulary\n",
    "\n",
    "3. A function to vectorize a sentence in the fashion described above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}\n",
    "    \n",
    "    def fit(self, text):\n",
    "        unique_words = set()  # Use a set to store unique words\n",
    "\n",
    "        for sentence in text:\n",
    "            words = sentence.split()\n",
    "            unique_words.update(words)\n",
    "        \n",
    "        sorted_words = sorted(unique_words)\n",
    "        sorted_words = enumerate(sorted_words) # Enumerate the words to get index\n",
    "        \n",
    "        for index, word in sorted_words:\n",
    "                self.vocabulary[word] = index\n",
    "        \n",
    "    def vectorize(self, sentence):\n",
    "        vector_size = len(self.vocabulary)\n",
    "        vector = np.zeros(vector_size, dtype=int)\n",
    "        words = sentence.split()\n",
    "        \n",
    "        for word in words:\n",
    "            if word in self.vocabulary:\n",
    "                index = self.vocabulary[word]\n",
    "                vector[index] += 1\n",
    "        \n",
    "        return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sanity check, we have manually set the vocabulary of your `BagOfWords` object to the vocabulary of the example above, and check that the vectorization of the sentence is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized sentence: [2 1 1 1 1]\n",
      "Vectorized sentence2: [1 1 1 0 0]\n",
      "Vectorized sentence3: [3 2 1 2 1]\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "bow.vocabulary = {\"the\": 0, \"cat\": 1, \"sat\": 2, \"on\": 3, \"mat\": 4}\n",
    "sentence1 = \"the cat sat on the mat\"\n",
    "sentence2 = \"the cat sat\"\n",
    "sentence3 = \"the the cat sat cat on on the mat\"\n",
    "\n",
    "vector = bow.vectorize(sentence1)\n",
    "vector2 = bow.vectorize(sentence2)\n",
    "vector3 = bow.vectorize(sentence3)\n",
    "\n",
    "print(\"Vectorized sentence:\", vector)\n",
    "print(\"Vectorized sentence2:\", vector2)\n",
    "print(\"Vectorized sentence3:\", vector3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3257, 8515) (374, 8515) (1421, 8515)\n"
     ]
    }
   ],
   "source": [
    "bow = BagOfWords()\n",
    "bow.fit(train_data['processed_text'])\n",
    "\n",
    "train_data_vectors = np.array([bow.vectorize(text) for text in train_data['processed_text']])\n",
    "validation_data_vectors = np.array([bow.vectorize(text) for text in validation_data['processed_text']])\n",
    "test_data_vectors = np.array([bow.vectorize(text) for text in test_data['processed_text']])\n",
    "\n",
    "print(train_data_vectors.shape, validation_data_vectors.shape, test_data_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "Now that we have vectorized our sentences, we can implement our Naive Bayes model. The Naive Bayes model is based off of the Bayes Theorem:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class $c$ that maximizes $P(c \\mid x)$, so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "We can then use the Naive Bayes assumption to simplify this:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i \\mid c)\n",
    "$$\n",
    "\n",
    "Where $x_i$ is the $i^{\\text{th}}$ word in our sentence.\n",
    "\n",
    "All of these probabilities can be estimated from our training data. We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i \\mid c)$ by counting the number of times the $i^{\\text{th}}$ word in our vocabulary appears in sentences of class $c$, and dividing by the total number of words in sentences of class $c$.\n",
    "\n",
    "It would help to apply logarithms to the above equation so that we translate the product into a sum, and avoid underflow errors. This will give us the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\log P(x_i \\mid c)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = None\n",
    "        self.likelihoods = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Get unique classes and their counts\n",
    "        unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "        self.classes = unique_labels\n",
    "        num_classes = unique_labels.shape[0]\n",
    "        self.class_priors = label_counts / y.shape[0]\n",
    "        num_features = X.shape[1]\n",
    "        self.likelihoods = np.zeros((num_classes, num_features))\n",
    "\n",
    "        # Calculate likelihoods for each class\n",
    "        for i, label in enumerate(self.classes):\n",
    "            xclass = X[y == label] \n",
    "            word_count = xclass.sum() \n",
    "            class_feature_sums = xclass.sum(axis=0)\n",
    "            smoothed_feature_sums = class_feature_sums + 1\n",
    "            denominator = word_count + num_features\n",
    "            self.likelihoods[i, :] = smoothed_feature_sums / denominator\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Initialize log probabilities for each sample and class\n",
    "        num_samples = X.shape[0]\n",
    "        num_classes = len(self.classes)\n",
    "        log_probabilities = np.zeros((num_samples, num_classes))\n",
    "\n",
    "        for i, class_label in enumerate(self.classes):\n",
    "            log_prior = np.log(self.class_priors[i])\n",
    "            log_likelihood = np.log(self.likelihoods[i, :])\n",
    "            log_probabilities[:, i] = log_prior + np.dot(X, log_likelihood.T)\n",
    "\n",
    "        predicted_classes = np.argmax(log_probabilities, axis=1)\n",
    "        \n",
    "        return self.classes[predicted_classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a Naive Bayes model on the training data, and generate predictions for the Validation Set.\n",
    "\n",
    "Reported below is the Accuracy, Precision, Recall, and F1 score of this model on the validation data as well as the Confusion Matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultinomialNaiveBayes()\n",
    "model.fit(train_data_vectors, train_data['label'].values)\n",
    "validation_predictions = model.predict(validation_data_vectors)\n",
    "validation_predictions = np.array(validation_predictions)\n",
    "\n",
    "validation_label = validation_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.650 \n",
      "\n",
      "Precision:\n",
      "Class 0: 0.632\n",
      "Class 1: 0.746\n",
      "Class 2: 1.000\n",
      "Class 3: 0.614\n",
      "\n",
      "Recall:\n",
      "Class 0: 0.881\n",
      "Class 1: 0.454\n",
      "Class 2: 0.143\n",
      "Class 3: 0.607\n",
      "\n",
      "F1 Score:\n",
      "Class 0: 0.736\n",
      "Class 1: 0.564\n",
      "Class 2: 0.250\n",
      "Class 3: 0.610\n",
      "\n",
      "Confusion Matrix:\n",
      "[[141   7   0  12]\n",
      " [ 38  44   0  15]\n",
      " [ 15   2   4   7]\n",
      " [ 29   6   0  54]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "classes = np.unique(validation_label)  \n",
    "\n",
    "accuracy = accuracy_score(validation_label, validation_predictions)\n",
    "precision = precision_score(validation_label, validation_predictions, average= None, labels=classes)\n",
    "recall = recall_score(validation_label, validation_predictions, average= None, labels=classes)\n",
    "f1score = f1_score(validation_label, validation_predictions, average= None, labels=classes)\n",
    "conf_matrix = confusion_matrix(validation_label, validation_predictions)\n",
    "\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.3f} \\n\")\n",
    "\n",
    "print(\"Precision:\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"Class {classes[i]}: {precision[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Recall:\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"Class {classes[i]}: {recall[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"F1 Score:\")\n",
    "for i in range(len(classes)):\n",
    "    print(f\"Class {classes[i]}: {f1score[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "confusionMatrix = confusion_matrix(validation_label, validation_predictions, labels=classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusionMatrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Implementation with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultinomialNaiveBayes()\n",
    "model.fit(train_data_vectors, train_data['label'].values)\n",
    "test_predictions = model.predict(test_data_vectors)\n",
    "test_predictions = np.array(test_predictions)\n",
    "\n",
    "test_label = test_data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.652 \n",
      "\n",
      "Precision:\n",
      "Class 0: 0.615\n",
      "Class 1: 0.772\n",
      "Class 2: 0.667\n",
      "Class 3: 0.662\n",
      "\n",
      "Recall:\n",
      "Class 0: 0.898\n",
      "Class 1: 0.483\n",
      "Class 2: 0.114\n",
      "Class 3: 0.626\n",
      "\n",
      "F1 Score:\n",
      "Class 0: 0.730\n",
      "Class 1: 0.595\n",
      "Class 2: 0.194\n",
      "Class 3: 0.643\n",
      "\n",
      "Confusion Matrix:\n",
      "[[501  19   2  36]\n",
      " [120 173   2  63]\n",
      " [ 74  12  14  23]\n",
      " [120  20   3 239]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_classes = np.unique(test_label)  \n",
    "\n",
    "new_accuracy = accuracy_score(test_label, test_predictions)\n",
    "new_precision = precision_score(test_label, test_predictions, average= None, labels=new_classes)\n",
    "new_recall = recall_score(test_label, test_predictions, average= None, labels=new_classes)\n",
    "new_f1score = f1_score(test_label, test_predictions, average= None, labels=new_classes)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Overall Accuracy: {new_accuracy:.3f} \\n\")\n",
    "\n",
    "print(\"Precision:\")\n",
    "for i in range(len(new_classes)):\n",
    "    print(f\"Class {new_classes[i]}: {new_precision[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"Recall:\")\n",
    "for i in range(len(new_classes)):\n",
    "    print(f\"Class {new_classes[i]}: {new_recall[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"F1 Score:\")\n",
    "for i in range(len(new_classes)):\n",
    "    print(f\"Class {new_classes[i]}: {new_f1score[i]:.3f}\")\n",
    "print()\n",
    "\n",
    "newconfusionMatrix = confusion_matrix(test_label, test_predictions, labels=new_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(newconfusionMatrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementing Naive Bayes using sklearn\n",
    "\n",
    "In this section, we compare the manual implementations with `sklearn`'s implementations of both of the Naive Bayes models we have covered above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of accuracy scores between manual and sklearn Bernoulli Naive Bayes:\n",
      "\n",
      "Sklearn Bernoulli Naive Bayes Accuracy: 0.820\n",
      "Manual Bernoulli Naive Bayes Accuracy: 0.820\n",
      "\n",
      "\n",
      "Comparison of accuracy scores between manual and sklearn Multinomial Naive Bayes:\n",
      "\n",
      "Sklearn Multinomial Naive Bayes Accuracy for validation data set: 0.650\n",
      "Manual Multinomial Naive Bayes Accuracy for validation data set: 0.650\n",
      "\n",
      "Sklearn Multinomial Naive Bayes Accuracy for test data set: 0.652\n",
      "Manual Multinomial Naive Bayes Accuracy for test data set: 0.652\n"
     ]
    }
   ],
   "source": [
    "bernoulli = BernoulliNB() \n",
    "bernoulli.fit(X_train, y_train)\n",
    "y_pred_sklearn = bernoulli.predict(X_test)\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Comparison of accuracy scores between manual and sklearn Bernoulli Naive Bayes:\")\n",
    "print()\n",
    "print(f\"Sklearn Bernoulli Naive Bayes Accuracy: {accuracy_sklearn:.3f}\")\n",
    "accuracy_own = accuracy_score(y_test, y_predicted)\n",
    "print(f\"Manual Bernoulli Naive Bayes Accuracy: {accuracy_own:.3f}\")\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"Comparison of accuracy scores between manual and sklearn Multinomial Naive Bayes:\")\n",
    "print()\n",
    "\n",
    "multi_train = train_data['label'].values  \n",
    "multi_validation = validation_data['label'].values\n",
    "multi_test = test_data['label'].values\n",
    "\n",
    "multinomial = MultinomialNB()\n",
    "multinomial.fit(train_data_vectors, multi_train)\n",
    "\n",
    "accuracy_sklearn_validation = accuracy_score(multi_validation, multinomial.predict(validation_data_vectors)) \n",
    "accuracy_sklearn_test = accuracy_score(multi_test, multinomial.predict(test_data_vectors))  \n",
    "accuracy_own_validation = accuracy_score(multi_validation, validation_predictions)  \n",
    "accuracy_own_accuracy_test = accuracy_score(multi_test, test_predictions)  \n",
    "\n",
    "print(f\"Sklearn Multinomial Naive Bayes Accuracy for validation data set: {accuracy_sklearn_validation:.3f}\")\n",
    "print(f\"Manual Multinomial Naive Bayes Accuracy for validation data set: {accuracy_own_validation:.3f}\\n\")\n",
    "print(f\"Sklearn Multinomial Naive Bayes Accuracy for test data set: {accuracy_sklearn_test:.3f}\")\n",
    "print(f\"Manual Multinomial Naive Bayes Accuracy for test data set: {accuracy_own_accuracy_test:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "#### The factors considered when determining which dataset is more suitable for **Multinomial Naive Bayes** and which is better suited for **Bernoulli Naive Bayes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluating the choice of Naive Bayes classifiers, I considered two key factors. Firstly, the type of output plays a crucial role; Bernoulli Naive Bayes is optimal for binary outcomes, while Multinomial Naive Bayes is more suitable for multiclass problems. Secondly, the nature of the features also influences the decision. If the features are binary or categorical, Bernoulli Naive Bayes is the appropriate choice. Conversely, for text data characterized by word counts or frequencies, Multinomial Naive Bayes proves to be the better option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Golf dataset involves binary outcomes (0 or 1) representing whether a person plays golf or not, based on categorical features like Outlook, Windy, Holiday, etc. Given that the Bernoulli Naive Bayes classifier is ideal for binary classification with binary features, it's a suitable choice for predicting whether golf will be played, as it focuses on the presence or absence of specific conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tweet evaluation dataset consists of multiple classes (0, 1, 2, 3), representing different sentiment or evaluation scores for tweets. The Multinomial Naive Bayes classifier is well-suited for this type of categorical data, especially when dealing with more than two classes. Its effectiveness with text data stems from its reliance on word frequencies, which significantly impact classification outcomes. In the context of sentiment analysis, certain words or phrases can serve as strong indicators of sentiment, significantly impacting classification accuracy. This makes the Multinomial Naive Bayes model an ideal choice for analyzing tweet sentiments, as the frequency of specific words can greatly influence the predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
